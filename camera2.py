#!/usr/bin/env python3
"""
ë“€ì–¼ ì¹´ë©”ë¼ ì‹¤ì‹œê°„ ìœµí•© ë·°ì–´ (ìˆœìˆ˜ GStreamer + OpenCV)

- IMX219 ì¹´ë©”ë¼ 2ëŒ€ë¥¼ libcamerasrcë¡œ ì¡ìŒ (ë„¤ê°€ ì“°ë˜ camera-name ê·¸ëŒ€ë¡œ ì‚¬ìš©)
- GStreamer íŒŒì´í”„ë¼ì¸ì—ì„œ appsinkë¡œ í”„ë ˆì„ì„ ê°€ì ¸ì™€ì„œ
- OpenCVë¡œ íŠ¹ì§•ì  ë§¤ì¹­ + í˜¸ëª¨ê·¸ë˜í”¼ + ì›Œí•‘ + ê°„ë‹¨ ë¸”ë Œë”©
- cam0 / cam1 / pano ì„¸ ì°½ì„ ë„ì›€
- ì´ˆê¸° í•œ ë²ˆë§Œ H(í˜¸ëª¨ê·¸ë˜í”¼) ê³„ì‚°í•´ì„œ ê³ ì •, í•„ìš”í•˜ë©´ R í‚¤ë¡œ ì¬ìº˜ë¦¬ë¸Œë ˆì´ì…˜
"""

import sys
import signal

import cv2 as cv
import numpy as np

import gi
gi.require_version('Gst', '1.0')
from gi.repository import Gst  # type: ignore

# GStreamer ì´ˆê¸°í™”
Gst.init(None)


# =====================================
# 0. ì¹´ë©”ë¼ ì •ë³´ (ë„¤ GStreamer ì½”ë“œ ê·¸ëŒ€ë¡œ)
# =====================================
CAMERAS = [
    {
        'name': 'Camera 0 (i2c@80000)',
        'device': '/base/axi/pcie@120000/rp1/i2c@80000/imx219@10',
    },
    {
        'name': 'Camera 1 (i2c@88000)',
        'device': '/base/axi/pcie@120000/rp1/i2c@88000/imx219@10',
    }
]


# ============================
# 1. ë””ë²„ê·¸ìš© ë§¤ì¹­ ì‹œê°í™”
# ============================
def draw_matches(img1, kp1, img2, kp2, matches, max_num=50):
    matches_to_draw = sorted(matches, key=lambda m: m.distance)[:max_num]
    dbg = cv.drawMatches(
        img1, kp1, img2, kp2, matches_to_draw, None,
        flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
    )
    cv.imshow("matches", dbg)


# ===================================
# 2. íŠ¹ì§•ì  ê²€ì¶œ + ë””ìŠ¤í¬ë¦½í„° + ë§¤ì¹­
# ===================================
def detect_and_match_features(img1, img2,
                              detector_type="sift",
                              ratio_test=0.75):
    if detector_type.lower() == "sift":
        if not hasattr(cv, "SIFT_create"):
            raise RuntimeError("ì´ OpenCV ë¹Œë“œì—ëŠ” SIFTê°€ ì—†ìŠµë‹ˆë‹¤.")
        sift = cv.SIFT_create()
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” detector_type")

    kp1, des1 = sift.detectAndCompute(img1, None)
    kp2, des2 = sift.detectAndCompute(img2, None)

    if des1 is None or des2 is None:
        return kp1, kp2, []

    FLANN_INDEX_KDTREE = 1
    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
    search_params = dict(checks=50)
    flann = cv.FlannBasedMatcher(index_params, search_params)

    matches_knn = flann.knnMatch(des1, des2, k=2)

    good_matches = []
    for m, n in matches_knn:
        if m.distance < ratio_test * n.distance:
            good_matches.append(m)

    return kp1, kp2, good_matches


# ==========================
# 3. RANSACìœ¼ë¡œ í˜¸ëª¨ê·¸ë˜í”¼
# ==========================
def estimate_homography(kp1, kp2, matches,
                        ransac_thresh=4.0):
    if len(matches) < 4:
        raise RuntimeError("ë§¤ì¹­ì ì´ ë„ˆë¬´ ì ì–´ì„œ í˜¸ëª¨ê·¸ë˜í”¼ ê³„ì‚° ë¶ˆê°€")

    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])
    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])

    H, mask = cv.findHomography(pts2, pts1, cv.RANSAC, ransac_thresh)
    if H is None or mask is None:
        raise RuntimeError("í˜¸ëª¨ê·¸ë˜í”¼ ê³„ì‚° ì‹¤íŒ¨")

    inliers = [matches[i] for i in range(len(matches)) if mask[i] != 0]
    return H, inliers, mask


# ===================================
# 4. í˜¸ëª¨ê·¸ë˜í”¼ ì›Œí•‘ + ê³µí†µ ìº”ë²„ìŠ¤ ê³„ì‚°
# ===================================
def warp_to_common_canvas(img1, img2, H):
    h1, w1 = img1.shape[:2]
    h2, w2 = img2.shape[:2]

    corners1 = np.float32([[0, 0],
                           [w1, 0],
                           [w1, h1],
                           [0, h1]]).reshape(-1, 1, 2)
    corners2 = np.float32([[0, 0],
                           [w2, 0],
                           [w2, h2],
                           [0, h2]]).reshape(-1, 1, 2)

    warped_corners2 = cv.perspectiveTransform(corners2, H)

    all_corners = np.concatenate((corners1, warped_corners2), axis=0)
    [x_min, y_min] = np.int32(all_corners.min(axis=0).ravel() - 0.5)
    [x_max, y_max] = np.int32(all_corners.max(axis=0).ravel() + 0.5)

    translation = [-x_min, -y_min]
    T = np.array([[1, 0, translation[0]],
                  [0, 1, translation[1]],
                  [0, 0, 1]], dtype=np.float32)

    pano_w = x_max - x_min
    pano_h = y_max - y_min

    img1_warp = cv.warpPerspective(img1, T, (pano_w, pano_h))
    img2_warp = cv.warpPerspective(img2, T @ H, (pano_w, pano_h))

    mask1 = np.full((h1, w1), 255, np.uint8)
    mask2 = np.full((h2, w2), 255, np.uint8)

    mask1_warp = cv.warpPerspective(mask1, T, (pano_w, pano_h))
    mask2_warp = cv.warpPerspective(mask2, T @ H, (pano_w, pano_h))

    return img1_warp, img2_warp, mask1_warp, mask2_warp, (pano_w, pano_h)


# =========================================
# 5. ê°„ë‹¨ ë¸”ë Œë”© (cv.detail ì—†ì´)
# =========================================
def simple_blend(img1_warp, img2_warp, mask1_warp, mask2_warp):
    pano_h, pano_w = img1_warp.shape[:2]
    pano = np.zeros((pano_h, pano_w, 3), dtype=np.uint8)

    m1 = mask1_warp > 0
    m2 = mask2_warp > 0
    overlap = m1 & m2
    only1 = m1 & (~overlap)
    only2 = m2 & (~overlap)

    pano[only1] = img1_warp[only1]
    pano[only2] = img2_warp[only2]

    if np.any(overlap):
        pano[overlap] = (
            0.5 * img1_warp[overlap].astype(np.float32)
            + 0.5 * img2_warp[overlap].astype(np.float32)
        ).astype(np.uint8)

    return pano


# =========================================
# 6-A. ì „ì²´ ìŠ¤í‹°ì¹­ íŒŒì´í”„ë¼ì¸ (ë§¤ í”„ë ˆì„ ë²„ì „) - í•„ìš”ì‹œ ë””ë²„ê·¸ìš©
# =========================================
def stitch_two_images(img1, img2, debug=False):
    kp1, kp2, matches = detect_and_match_features(img1, img2)

    if debug:
        print(f"ì´ ë§¤ì¹­ ìˆ˜: {len(matches)}")
        if len(matches) > 0:
            draw_matches(img1, kp1, img2, kp2, matches)

    if len(matches) < 4:
        raise RuntimeError("ìœ íš¨í•œ ë§¤ì¹­ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    H, inliers, _ = estimate_homography(kp1, kp2, matches)

    if debug:
        print(f"RANSAC ì¸ë¼ì´ì–´ ìˆ˜: {len(inliers)}")

    img1_warp, img2_warp, mask1_warp, mask2_warp, _ = \
        warp_to_common_canvas(img1, img2, H)

    pano = simple_blend(img1_warp, img2_warp, mask1_warp, mask2_warp)
    return pano


# =========================================
# 6-B. ì´ë¯¸ ì•Œê³  ìˆëŠ” Hë¡œ ìŠ¤í‹°ì¹­ (ê³ ì • H ë²„ì „)
# =========================================
def stitch_with_fixed_homography(img1, img2, H):
    """
    ì´ë¯¸ ê³„ì‚°ëœ Hë¥¼ ì‚¬ìš©í•´ì„œ ë‘ ì´ë¯¸ì§€ë¥¼ warp + blendë§Œ ìˆ˜í–‰
    """
    img1_warp, img2_warp, mask1_warp, mask2_warp, _ = \
        warp_to_common_canvas(img1, img2, H)

    pano = simple_blend(img1_warp, img2_warp, mask1_warp, mask2_warp)
    return pano


# =========================================
# 6-C. ì´ˆê¸° í•œ ë²ˆë§Œ H ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
# =========================================
def compute_homography_once(img1, img2, debug=False):
    kp1, kp2, matches = detect_and_match_features(img1, img2)

    if debug:
        print(f"ì´ˆê¸° ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë§¤ì¹­ ìˆ˜: {len(matches)}")
        if len(matches) > 0:
            draw_matches(img1, kp1, img2, kp2, matches)

    if len(matches) < 4:
        raise RuntimeError("ì´ˆê¸° H ê³„ì‚°: ìœ íš¨í•œ ë§¤ì¹­ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    H, inliers, _ = estimate_homography(kp1, kp2, matches)

    if debug:
        print(f"ì´ˆê¸° RANSAC ì¸ë¼ì´ì–´ ìˆ˜: {len(inliers)}")

    return H


# =========================================
# 7. GStreamer íŒŒì´í”„ë¼ì¸ + appsink
# =========================================
def create_gst_pipeline(camera_device, width=640, height=480, sink_name="sink"):
    """
    libcamerasrc camera-name=... !
      video/x-raw,width=640,height=480,format=NV21 !
      videoconvert !
      video/x-raw,format=BGR !
      appsink name=sink ...
    """
    pipeline_desc = (
        f"libcamerasrc camera-name={camera_device} ! "
        f"video/x-raw,width={width},height={height},format=NV21 ! "
        "videoconvert ! "
        "video/x-raw,format=BGR ! "
        f"appsink name={sink_name} max-buffers=1 drop=true sync=false"
    )
    pipeline = Gst.parse_launch(pipeline_desc)
    if pipeline is None:
        raise RuntimeError("GStreamer íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨")

    sink = pipeline.get_by_name(sink_name)
    if sink is None:
        raise RuntimeError("appsinkë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    return pipeline, sink


def gst_sample_to_ndarray(sample):
    """
    appsinkì—ì„œ ë°›ì€ Gst.Sample â†’ numpy ë°°ì—´(BGR)ë¡œ ë³€í™˜
    """
    buf = sample.get_buffer()
    caps = sample.get_caps()
    s = caps.get_structure(0)
    width = s.get_value('width')
    height = s.get_value('height')

    success, map_info = buf.map(Gst.MapFlags.READ)
    if not success:
        return None

    try:
        data = map_info.data
        # BGR, 3ì±„ë„
        frame = np.ndarray(
            (height, width, 3),
            dtype=np.uint8,
            buffer=data
        )
        # ë³µì‚¬í•´ì„œ ë°˜í™˜ (GStreamer ë²„í¼ ë¼ì´í”„íƒ€ì„ê³¼ ë¶„ë¦¬)
        return frame.copy()
    finally:
        buf.unmap(map_info)


# =========================================
# 8. ë©”ì¸ ë£¨í”„
# =========================================
def main_video():
    print("GStreamer ë“€ì–¼ ì¹´ë©”ë¼ + ì‹¤ì‹œê°„ ìŠ¤í‹°ì¹­ ì‹œì‘ ì¤€ë¹„...")

    cam0_dev = CAMERAS[0]['device']
    cam1_dev = CAMERAS[1]['device']

    # ê° ì¹´ë©”ë¼ì— ëŒ€í•œ íŒŒì´í”„ë¼ì¸ + appsink ìƒì„±
    pipeline0, sink0 = create_gst_pipeline(cam0_dev, 640, 480, "sink0")
    pipeline1, sink1 = create_gst_pipeline(cam1_dev, 640, 480, "sink1")

    # ì¬ìƒ ì‹œì‘
    pipeline0.set_state(Gst.State.PLAYING)
    pipeline1.set_state(Gst.State.PLAYING)

    print("âœ… ë‘ ì¹´ë©”ë¼ íŒŒì´í”„ë¼ì¸ PLAYING ìƒíƒœë¡œ ì§„ì…")
    print("ESC: ì¢…ë£Œ / R: H ë‹¤ì‹œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜")

    H_fixed = None   # â† ì´ˆê¸°ì—ëŠ” ì—†ìŒ, í•œ ë²ˆ ê³„ì‚° í›„ ê³ ì •

    try:
        while True:
            # ê° ì¹´ë©”ë¼ì—ì„œ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ: 1ì´ˆ)
            sample0 = sink0.emit("try-pull-sample", 1_000_000_000)
            sample1 = sink1.emit("try-pull-sample", 1_000_000_000)

            if sample0 is None or sample1 is None:
                print("âš  ìƒ˜í”Œì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (None). ê³„ì† ì‹œë„...")
                continue

            frame0 = gst_sample_to_ndarray(sample0)
            frame1 = gst_sample_to_ndarray(sample1)

            if frame0 is None or frame1 is None:
                print("âš  í”„ë ˆì„ ë³€í™˜ ì‹¤íŒ¨. ê³„ì† ì‹œë„...")
                continue

            # ---------- ê³ ì • H ë¡œì§ ----------
            # 1) ì•„ì§ H_fixedê°€ ì—†ìœ¼ë©´, í•œ ë²ˆë§Œ ê³„ì‚°
            if H_fixed is None:
                try:
                    print("ğŸ“ ì´ˆê¸° H ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹œë„ ì¤‘...")
                    H_fixed = compute_homography_once(frame0, frame1, debug=False)
                    print("âœ… ì´ˆê¸° H ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!")
                except Exception as e:
                    print("ì´ˆê¸° H ê³„ì‚° ì‹¤íŒ¨, ë‹¤ìŒ í”„ë ˆì„ì—ì„œ ë‹¤ì‹œ ì‹œë„:", e)
                    # H_fixed ëª» êµ¬í–ˆìœ¼ë©´ ê·¸ëƒ¥ ì›ë³¸ë§Œ ë³´ì—¬ì£¼ê³  ë„˜ì–´ê°
                    cv.imshow("cam0", frame0)
                    cv.imshow("cam1", frame1)
                    key = cv.waitKey(1) & 0xFF
                    if key == 27:  # ESC
                        break
                    elif key == ord('r') or key == ord('R'):
                        print("ğŸ”„ H ì´ˆê¸°í™” ìš”ì²­ (ì´ë¯¸ None ìƒíƒœ).")
                        H_fixed = None
                    continue

            # 2) H_fixedê°€ ìˆìœ¼ë©´, ì´ê±¸ë¡œë§Œ warp + blend
            pano = None
            try:
                pano = stitch_with_fixed_homography(frame0, frame1, H_fixed)
            except Exception as e:
                print("ê³ ì • Hë¡œ ìŠ¤í‹°ì¹­ ì‹¤íŒ¨:", e)
                pano = None
            # ---------- ê³ ì • H ë¡œì§ ë ----------

            cv.imshow("cam0", frame0)
            cv.imshow("cam1", frame1)
            if pano is not None:
                cv.imshow("pano", pano)

            key = cv.waitKey(1) & 0xFF
            if key == 27:  # ESC
                break
            elif key == ord('r') or key == ord('R'):
                # Hë¥¼ ë‹¤ì‹œ ì¡ê³  ì‹¶ì„ ë•Œ (ì¹´ë©”ë¼ ìœ„ì¹˜ ë°”ê¿¨ì„ ë•Œ ë“±)
                print("ğŸ”„ H ì´ˆê¸°í™”. ë‹¤ìŒ í”„ë ˆì„ì—ì„œ ë‹¤ì‹œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜í•©ë‹ˆë‹¤.")
                H_fixed = None

    except KeyboardInterrupt:
        print("\n[Ctrl+C ê°ì§€] ì¢…ë£Œí•©ë‹ˆë‹¤.")

    # ì •ë¦¬
    pipeline0.set_state(Gst.State.NULL)
    pipeline1.set_state(Gst.State.NULL)
    cv.destroyAllWindows()


def main():
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬: Ctrl+C ì‹œ ê·¸ë‚˜ë§ˆ ê¹¨ë—í•˜ê²Œ ì¢…ë£Œ
    def signal_handler(sig, frame):
        print("\n[ì‹œê·¸ë„ ê°ì§€] ì¢…ë£Œí•©ë‹ˆë‹¤.")
        cv.destroyAllWindows()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    main_video()


if __name__ == "__main__":
    main()
